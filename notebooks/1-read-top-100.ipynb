{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from random import uniform\n",
    "from os.path import exists\n",
    "import re\n",
    "import time\n",
    "import csv\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_100_url = 'https://www.amazon.com.br/gp/bestsellers/books/ref=zg_bs_pg_2?ie=UTF8&pg='\n",
    "top_100_url_page_number = 1\n",
    "book_reviews_url_begin = 'https://www.amazon.com.br/texto/product-reviews/'\n",
    "book_reviews_url_end = '/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews&pageNumber='\n",
    "book_reviews_url_page_number = 1\n",
    "books_with_reviews = 0\n",
    "books_saved = 0\n",
    "csv_file_path = 'datasets/original_complete_csv_file.csv'\n",
    "log_file_path = 'log/read_top_100_log.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the current time and date when the script starts\n",
    "def start_counting_time(log_file):\n",
    "    # datetime object containing current date and time\n",
    "    start_time = datetime.datetime.now()\n",
    "    # dd/mm/YY H:M:S\n",
    "    start_time_string = start_time.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "    print(\"Beginning web scrapping at \", start_time_string, \"\\n\")\t\n",
    "    log_file.write(\"Beginning web scrapping at \" + start_time_string + \"\\n\")\t\n",
    "        \n",
    "    return start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the current time and date when the script ends\n",
    "def finish_counting_time(start_time, books_with_reviews, log_file, books_saved):\n",
    "    # datetime object containing current date and time\n",
    "    finish_time = datetime.datetime.now()\n",
    "    # dd/mm/YY H:M:S\n",
    "    finish_time_string = finish_time.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "    log_file.write(\"\\nFinished web scrapping at \" + finish_time_string + \"\\n\")\t\n",
    "    print(\"\\nFinished web scrapping at\", finish_time_string)\t\n",
    "    \n",
    "    difference = (finish_time - start_time).total_seconds()\n",
    "    log_file.write(str(books_with_reviews) + \" books with reviews were found among top 100 books, \" + str(books_saved) + \" where saved and the script took \" + str(difference) + \" seconds to complete.\\n\")\n",
    "    print(str(books_with_reviews) + \" books with reviews were found among top 100 books, \" + str(books_saved) + \" where saved and the script took \" + str(difference) + \" seconds to complete.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the review line in the csv file\n",
    "def write_review(review, writer, book_id, review_number, log_file):\n",
    "\n",
    "    # Try to get the rating information from the review\n",
    "    match = re.search('a-star-\\d', str(review))\n",
    "    if match:\n",
    "        rating = match.group(0)[7:]\n",
    "    else:\n",
    "        rating = ''    \n",
    "\n",
    "    # Get the review title\n",
    "    anchor = review.find_all(\"a\", {\"class\": \"review-title\"})\n",
    "    if anchor:\n",
    "        title = anchor[0].span.text.replace(';', ',')\n",
    "        title = str(title).replace(';)', ':)')\n",
    "        title = str(title).replace(';', ',')\n",
    "        title = str(title).replace('\"', '')\n",
    "        title = title.strip()\n",
    "    else:\n",
    "        title = ''\n",
    "    # Get the review id\n",
    "    review_id = review.get('id')\n",
    "\n",
    "    # Get the review text\n",
    "    review_text_content = review.select('.review-text-content')\n",
    "    spam_quantity = len(review_text_content[0].select('span'))\n",
    "\n",
    "    log_file.write('\\t\\tread review #' + str(review_number) + ' whose id is ' + str(review_id) + '\\n')\n",
    "    print('\\t\\tread review #' + str(review_number) + ' whose id is ' + str(review_id))\n",
    "\n",
    "    # If the review text is not empty, try to sanitize the text\n",
    "    if spam_quantity != 0:\n",
    "        text = review_text_content[0].select('span')[spam_quantity - 1]\n",
    "        text = str(text).replace(';', ',')\n",
    "        text = str(text).replace('\"', '')\n",
    "        text = str(text).replace('<span>', '')\n",
    "        text = str(text).replace('</span>', '')\n",
    "        text = str(text).replace('<br/>', '')\n",
    "        text = str(text).replace('</br>', '')\n",
    "        text = str(text).replace('<br>', '')\n",
    "        text = text.strip()\n",
    "        \n",
    "        writer.writerow([book_id + ';' + review_id + ';' + rating + ';' + title + ';' + text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all book ids already stored in the file\n",
    "def get_books_from_file(csv_file_path, log_file):\n",
    "    \n",
    "    # Check if the file exists\n",
    "    file_exists = exists(csv_file_path)\n",
    "    # Create an empty set of books\n",
    "    books = set()\n",
    "\n",
    "    if file_exists:\n",
    "        # Open the file to read\n",
    "        csv_file = open(csv_file_path, 'r')\n",
    "\n",
    "        # Read all lines\n",
    "        lines = csv_file.readlines()\n",
    "        \n",
    "        log_file.write(\"\\nBooks already found in the csv file:\\n\")\n",
    "        print(\"\\nBooks already found in the csv file:\")\n",
    "\n",
    "        # For each line in the file\n",
    "        for line in lines:\n",
    "            # Search for first occurance of digits (book_id) \n",
    "            match = re.search(r'\\d+', str(line))\n",
    "            if match:\n",
    "                book_id = match.group()\n",
    "                # Add the book_id to the set\n",
    "                books.add(book_id)              \n",
    "\n",
    "        # Close the file\n",
    "        csv_file.close()\n",
    "\n",
    "    for book in books:\n",
    "        log_file.write(\"\\t\" + book + \"\\n\")\n",
    "        print(\"\\t\" + book)\n",
    "    \n",
    "    log_file.write(\"\\n \" + str(len(books)) + \" books where found.\\n\")\n",
    "    print(\"\\n \" + str(len(books)) + \" books where found.\\n\")\n",
    "    return books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the book reviews for that page\n",
    "def get_book_reviews(book_reviews_url_begin, book_id, book_reviews_url_end, book_reviews_url_page_number, books_with_reviews, log_file):\n",
    "    \n",
    "    log_file.write('\\tBook #' + str(books_with_reviews) + ' whose id is: ' + str(book_id) + ', reading page #' + str(book_reviews_url_page_number) + '\\n')\n",
    "    print('\\tBook #' + str(books_with_reviews) + ' whose id is: ' + str(book_id) + ', reading page #' + str(book_reviews_url_page_number))\n",
    "    \n",
    "    # Access the book page and get all the reviews\n",
    "    book_page = requests.get(book_reviews_url_begin + str(book_id) + book_reviews_url_end + str(book_reviews_url_page_number))\n",
    "    log_file.write(\"\\t\\tServer response status code: \" + str(book_page.status_code) + '\\n')\n",
    "    print(\"\\t\\tServer response status code: \" + str(book_page.status_code))\n",
    "\n",
    "    # If it gets a 503 or 404 error from Amazon, sleeps for some seconds and try again until it works\n",
    "    while (book_page.status_code != 200):\n",
    "        if book_page.status_code == 404:\n",
    "            log_file.write(\"\\t\\tGoing to try next book (404).\\n\")\n",
    "            print(\"\\t\\tGoing to try next book (404).\")\n",
    "            return False\n",
    "            \n",
    "        log_file.write(\"\\t\\tGoing to retry.\\n\")\n",
    "        print(\"\\t\\tGoing to retry.\")\n",
    "        time.sleep(uniform(1.0, 5.0))\n",
    "        book_page = requests.get(book_reviews_url_begin + str(book_id) + book_reviews_url_end + str(book_reviews_url_page_number))\n",
    "        log_file.write(\"\\t\\tServer response status code: \" + str(book_page.status_code) + '\\n')\n",
    "        print(\"\\t\\tServer response status code: \" + str(book_page.status_code))\n",
    "\n",
    "\n",
    "    book_soup = BeautifulSoup(book_page.text, 'html.parser')\n",
    "    \n",
    "    # Selecting the reviews from the page\n",
    "    reviews = book_soup.select('.review')\n",
    "    return reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_page(top_100_url, top_100_url_page_number, log_file):\n",
    "    \n",
    "    # Get the next 50 books \n",
    "    top_100_books_page = requests.get(top_100_url + str(top_100_url_page_number))\n",
    "    log_file.write(\"\\tServer response status code: \" + str(top_100_books_page.status_code) + '\\n')       \n",
    "    print(\"\\tServer response status code: \" + str(top_100_books_page.status_code))\n",
    "                     \n",
    "    # If it gets a 503 error from Amazon, sleeps for some seconds and try again until it works\n",
    "    while (top_100_books_page.status_code == 503):\n",
    "        log_file.write(\"\\tSever responded with 503, going to retry.\\n\")\n",
    "        print(\"\\tSever responded with 503, going to retry.\")\n",
    "        time.sleep(uniform(1.0, 5.0))\n",
    "        top_100_books_page = requests.get(top_100_url + str(top_100_url_page_number))\n",
    "\n",
    "    return top_100_books_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_books_information(top_100_books_page):\n",
    "    # Initializing the beautifulsoup for html manipulation \n",
    "    soup = BeautifulSoup(top_100_books_page.text, 'html.parser')\n",
    "        \n",
    "    # Selecting the top 100 books from the page\n",
    "    div_books = soup.select('.zg-grid-general-faceout')\n",
    "\n",
    "    return div_books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_book_id(div_book):\n",
    "    # Try to get the book id\n",
    "    m = re.search('pd_rd_i=\\d*', str(div_book))\n",
    "    book_id = m.group(0)[8:]\n",
    "\n",
    "    return book_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def book_has_reviews(div_book):\n",
    "    # Check if the book has reviews\n",
    "    r = re.search('a-icon-row', str(div_book))\n",
    "    # If the book has reviews, grab all of them\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File that will hold the log\n",
    "log_file = open(log_file_path, \"w\")\n",
    "# Start counting the time\n",
    "start_time = start_counting_time(log_file)\n",
    "# Get the books that are already stored in the csv file\n",
    "books_already_stored = get_books_from_file(csv_file_path, log_file)\n",
    "\n",
    "with open(csv_file_path, 'a', encoding='UTF8', newline='') as f:\n",
    "    writer = csv.writer(f, delimiter=';', quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "    # If the books set has no books, the file does not exist\n",
    "    if len(books_already_stored) == 0:\n",
    "        writer.writerow( ['book_id;review_id;review_rating;review_title;review_text;'] )\n",
    "\n",
    "    # Top 100 books are divided into 2 pages, this while loop will be exectuded twice\n",
    "    while (top_100_url_page_number < 3):\n",
    "        \n",
    "        top_100_books_page = get_next_page(top_100_url, top_100_url_page_number, log_file)\n",
    "        # If the request was succeded\n",
    "        if (top_100_books_page.status_code == 200):\n",
    "            \n",
    "            # Get all the divs of books\n",
    "            div_books = get_books_information(top_100_books_page)\n",
    "\n",
    "            # For each book found\n",
    "            for div_book in div_books:\n",
    "                \n",
    "                # Get the book id\n",
    "                book_id = get_book_id(div_book)\n",
    "                # Check if the book has reviews\n",
    "                has_reviews = book_has_reviews(div_book)\n",
    "                if has_reviews:\n",
    "                    # Increment the book with reviews counter\n",
    "                    books_with_reviews+=1 \n",
    "                    log_file.write('\\nGoing to read the book #' + str(books_with_reviews) + ' whose id is ' + str(book_id) + '\\n')\n",
    "                    print('\\nGoing to read the book #' + str(books_with_reviews) + ' whose id is ' + str(book_id))\n",
    "                    \n",
    "                    # Check if the book is already stored in the csv file\n",
    "                    if book_id in books_already_stored:\n",
    "                        log_file.write('\\tBook #' + str(books_with_reviews) + ' whose id is ' + str(book_id) + ' is already stored in the file.\\n')\n",
    "                        print('\\tBook #' + str(books_with_reviews) + ' whose id is ' + str(book_id) + ' is already stored in the file.')\n",
    "                        # Skip this book because it is already in the file\n",
    "                        continue\n",
    "\n",
    "                \n",
    "                    end = False    \n",
    "\n",
    "                    x = 1\n",
    "                    review_number = 0\n",
    "\n",
    "                    # While it finds reviews\n",
    "                    while not end:\n",
    "                    \n",
    "                        # Reads only 10 pages and the sleeps for 3 seconds, trying to avoid 503 back from Amazon\n",
    "                        for book_reviews_url_page_number in range(x, x + 10):    \n",
    "                            \n",
    "                            # Get the book reviews for this page\n",
    "                            reviews = get_book_reviews(book_reviews_url_begin, book_id, book_reviews_url_end, book_reviews_url_page_number, books_with_reviews, log_file)\n",
    "\n",
    "                            # If it doesn't find any reviews, it has reached the last review and might finish\n",
    "                            if not reviews:\n",
    "                                log_file.write('\\t\\tDidn\\'t find more reviews.\\n')\n",
    "                                print('\\t\\tDidn\\'t find more reviews.\\n')\n",
    "                                end = True\n",
    "                                break\n",
    "                            else:\n",
    "                                if review_number == 0: \n",
    "                                    books_saved += 1\n",
    "\n",
    "                                log_file.write('\\t\\tFound more ' + str(len(reviews)) + ' reviews.\\n')\n",
    "                                print('\\t\\tFound more ' + str(len(reviews)) + ' reviews.')\n",
    "                                # For each review found in the page, write the data into the csv file\n",
    "                                for review in reviews:\n",
    "                                    review_number+=1\n",
    "                                    #write_review(review, writer, book_id, review_number, log_file)\n",
    "                                    write_review(review, writer, book_id, review_number, log_file)\n",
    "                                    \n",
    "                        # Advance to the next page that hasn't been reached yet\n",
    "                        x+=10  \n",
    "                        # Sleeps for 3 seconds, trying to avoid 503 back from Amazon \n",
    "                        time.sleep(3)\n",
    "\n",
    "        top_100_url_page_number+=1\n",
    "\n",
    "# Finish counting the time\n",
    "finish_counting_time(start_time, books_with_reviews, log_file, books_saved)\n",
    "log_file.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "507a8b22a7c0b40ef0f08e0b9bf6dfcd321260259de865b53fc1aff31bf9277a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
